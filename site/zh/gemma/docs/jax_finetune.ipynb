{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### 版權 2024 Google LLC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_yUpPhqrRrK"
      },
      "source": [
        "# 使用 JAX 和 Flax 微調 Gemma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yDXE-RX835U"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/jax_finetune\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />檢視 ai.google.dev</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/doggy8088/generative-ai-docs/blob/main/site/zh/gemma/docs/jax_finetune.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />在 Google Colab 中執行</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/google/generative-ai-docs/main/site/en/gemma/docs/jax_finetune.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />在 Vertex AI 中開啟</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/doggy8088/generative-ai-docs/blob/main/site/zh/gemma/docs/jax_finetune.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />在 GitHub 上檢視原始程式碼</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUnQEMHBt3nc"
      },
      "source": [
        "## 概述\n",
        "\n",
        "Gemma 是一系列以 Google DeepMind Gemini 研究和技術為基礎的輕量級、最先進的開放大型語言模型。本教學課程示範如何使用 [Google DeepMind 的 `gemma` 函式庫](https://github.com/google-deepmind/gemma)、[JAX](https://jax.readthedocs.io)(高性能數值運算函式庫)、[Flax](https://flax.readthedocs.io)(基於 JAX 的神經網路函式庫)、[Chex](https://chex.readthedocs.io/en/latest/)(用於撰寫可靠 JAX 程式碼的公用程式函式庫)、[Optax](https://optax.readthedocs.io/en/latest/)(基於 JAX 的梯度處理和最佳化函式庫)、[MTNT (雜訊文字機器翻譯) 資料集](https://arxiv.org/abs/1809.00388) 來微調 Gemma 2B Instruct 模型，以進行英法翻譯任務。儘管本筆記本未直接使用 Flax，但 Flax 已用於建立 Gemma。\n",
        "\n",
        "`gemma` 函式庫使用 JAX、Flax、[Orbax](https://orbax.readthedocs.io/)(用於訓練公用程式 (例如檢查點) 的基於 JAX 的函式庫) 和 [SentencePiece](https://github.com/google/sentencepiece)(分詞器/反分詞器函式庫) 編寫。\n",
        "\n",
        "**注意：** 本筆記本在 Google Colab 中使用 A100 GPU 執行。免費的 Colab 硬體加速不足以執行本筆記本，因為它需要大量的主機記憶體，例如 A100 GPU (在 Colab Pro 中提供) 或至少 Google Cloud TPU v3-8。你可以使用提供免費 TPU v3-8 加速的 [Kaggle VM 筆記本](https://www.kaggle.com/)；或 [Google Cloud TPU](https://cloud.google.com/tpu?hl=en) 提供 TPU v3 和更高版本。目前，Google Colab 提供 TPU v2，這不足以滿足本教學的要求。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbRLI7Q4-8Ve"
      },
      "source": [
        "## 設定\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Ku4iK6PnC0"
      },
      "source": [
        "### 1. 為 Gemma 設定 Kaggle 存取權\n",
        "\n",
        "要完成本教學課程，需要先按照 [Gemma 設定](https://ai.google.dev/gemma/docs/setup) 中的設定說明進行操作，了解如何執行下列動作：\n",
        "\n",
        "* 在 [kaggle.com](https://www.kaggle.com/models/google/gemma/) 上取得 Gemma 存取權。\n",
        "* 選取具有足夠資源來執行 Gemma 模型的 Colab 執行時期。\n",
        "* 產生並設定 Kaggle 使用者名稱和 API 金鑰。\n",
        "\n",
        "完成 Gemma 設定後，請移至下一章節，設定 Colab 環境的環境變數。\n",
        "\n",
        "### 2. 設定環境變數\n",
        "\n",
        "設定 `KAGGLE_USERNAME` 和 `KAGGLE_KEY` 的環境變數。當出現「授予存取權？」訊息時，請同意提供機密存取權。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AVH6Y4k2964n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata # `userdata` is a Colab API.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1UE1CEnE9ql"
      },
      "source": [
        "### 3. 安裝 `gemma` 函式庫\n",
        "\n",
        "免費的 Colab 硬體加速目前 *不足* 以執行這個筆記本。如果你使用的是 [Colab 付費或 Colab 專業版](https://colab.research.google.com/signup)，請按一下「編輯」>「筆記本設定」> 選擇「A100 GPU」>「儲存」來啟用硬體加速。\n",
        "\n",
        "接下來，你需要從 [`github.com/google-deepmind/gemma`](https://github.com/google-deepmind/gemma) 安裝 Google DeepMind 的 `gemma` 函式庫。如果你收到關於「pip 的依存關係解析程式」的錯誤訊息，通常可以忽略它。\n",
        "\n",
        "**注意：** 透過安裝 `gemma`，你還會安裝 [`flax`](https://flax.readthedocs.io)、核心 [`jax`](https://jax.readthedocs.io)、[`optax`](https://optax.readthedocs.io/en/latest/)  (基於 JAX 的梯度處理和最佳化函式庫)、[`orbax`](https://orbax.readthedocs.io/) 和 [`sentencepiece`](https://github.com/google/sentencepiece)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XpSw-_4EEcoY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.4/244.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gemma (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires absl-py<2.0.0,>=0.9, but you have absl-py 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/google-deepmind/gemma.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mRkkT-iPYoq"
      },
      "source": [
        "### 4. 匯入函式庫\n",
        "\n",
        "本筆記本使用 [Flax](https://flax.readthedocs.io) (用於神經網路)、核心 [JAX](https://jax.readthedocs.io)，[SentencePiece](https://github.com/google/sentencepiece) (用於 Token 化)、[Chex](https://chex.readthedocs.io/en/latest/) (可靠 JAX 程式碼編寫的函式庫) 和 TensorFlow Datasets。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ChMf1H4mPVx_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import enum\n",
        "import re\n",
        "import string\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from gemma import params as params_lib\n",
        "from gemma import sampler as sampler_lib\n",
        "from gemma import transformer as transformer_lib\n",
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNgKIkxMOsit"
      },
      "source": [
        "## 載入 Gemma 模型\n",
        "\n",
        "使用 [`kagglehub.model_download`](https://github.com/Kaggle/kagglehub/blob/bddefc718182282882b72f814d407d89e5d178c4/src/kagglehub/models.py#L12) 載入 Gemma 模型，其有三個參數：\n",
        "\n",
        "- `句柄`：Kaggle 的模型句柄\n",
        "- `路徑`： (選擇性字串) 本地路徑\n",
        "- `強制下載`： (選擇性布林值) 強制重新下載模型\n",
        "\n",
        "**注意** ：請注意 `gemma-2b-it` 模型大小約為 3.7Gb。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X-i10429N-g2"
      },
      "outputs": [],
      "source": [
        "GEMMA_VARIANT = '2b-it' # @param ['2b', '2b-it'] {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j_QdPAGyO5zl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/google/gemma/flax/2b-it/2/download...\n",
            "100%|██████████| 3.67G/3.67G [00:26<00:00, 147MB/s]\n",
            "Extracting model files...\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "GEMMA_PATH = kagglehub.model_download(f'google/gemma/flax/{GEMMA_VARIANT}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cjnXlLkWcHIy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GEMMA_PATH: /root/.cache/kagglehub/models/google/gemma/flax/2b-it/2\n"
          ]
        }
      ],
      "source": [
        "print('GEMMA_PATH:', GEMMA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1HzOpDcM04q"
      },
      "source": [
        "**提示：** 上方的輸出路徑是模型權重和 tokenizer 本機儲存的位置，你稍後會用到它們。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ytvcJ8FPEMm"
      },
      "source": [
        "檢視模型權重和分詞器的存放位置，然後設定路徑變數。分詞器目錄會在你下載模型的主目錄中，而模型權重則會在子目錄中。例如：\n",
        "\n",
        "- `tokenizer.model` 檔案會在 `/LOCAL/PATH/TO/gemma/flax/2b-it/2` 中)。\n",
        "- 模型檢查點會在 `/LOCAL/PATH/TO/gemma/flax/2b-it/2/2b-it` 中)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JAwXvpzbuiB5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CKPT_PATH: /root/.cache/kagglehub/models/google/gemma/flax/2b-it/2/2b-it\n",
            "TOKENIZER_PATH: /root/.cache/kagglehub/models/google/gemma/flax/2b-it/2/tokenizer.model\n"
          ]
        }
      ],
      "source": [
        "CKPT_PATH = os.path.join(GEMMA_PATH, GEMMA_VARIANT)\n",
        "TOKENIZER_PATH = os.path.join(GEMMA_PATH, 'tokenizer.model')\n",
        "print('CKPT_PATH:', CKPT_PATH)\n",
        "print('TOKENIZER_PATH:', TOKENIZER_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U800JRcJVIlF"
      },
      "source": [
        "## 載入並準備 MTNT 資料集與 Gemma 分詞器\n",
        "\n",
        "你將使用 [MTNT (雜訊文本機器翻譯)](https://arxiv.org/abs/1809.00388) 資料集，它可以在 [TensorFlow 資料集](https://www.tensorflow.org/datasets/catalog/mtnt) 中取得。\n",
        "\n",
        "下載 MTNT 資料集的英語至法語資料集部分，然後取樣兩個範例。資料集中的每個範例包含兩個項目：「src」：原生的英文句子；與「dst」：對應的法文翻譯。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pg8SfQH0EcoY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset 35.08 MiB (download: 35.08 MiB, generated: 11.33 MiB, total: 46.41 MiB) to /root/tensorflow_datasets/mtnt/en-fr/1.0.0...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ec9a4a2b77f41e4a7435359338b140c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f799eec281194b80b8f260224df50ae3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4804ce26e0b84a5e8a9774bb5dcd1ebc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04b3ecfe7275446e816804c01da57572",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e2b3d7109ea47a4a28bf02f0683c50b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train examples...:   0%|          | 0/35692 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da26bf08c7be4a79b64e94b3f5ad028c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/mtnt/en-fr/1.0.0.incomplete6YJMND/mtnt-train.tfrecord*...:   0%|          …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c864366a50e94b4fbc38085f0f602dca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test examples...:   0%|          | 0/1020 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad5ce2585efc45899da7b33ac0663005",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/mtnt/en-fr/1.0.0.incomplete6YJMND/mtnt-test.tfrecord*...:   0%|          |…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "384b494a52a54757ab3fdefee721327c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating valid examples...:   0%|          | 0/811 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "943840b94fc64a05baf9c8e057dc3b85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/mtnt/en-fr/1.0.0.incomplete6YJMND/mtnt-valid.tfrecord*...:   0%|          …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset mtnt downloaded and prepared to /root/tensorflow_datasets/mtnt/en-fr/1.0.0. Subsequent calls will reuse this data.\n",
            "Example 0:\n",
            "dst: b'Le groupe de \" toutes les \\xc3\\xa9toiles potentielles de la conf\\xc3\\xa9rence de l\\'Est mais qui ne s\\'en sortent pas dans le groupe de l\\'Ouest \".'\n",
            "src: b'The group of \\xe2\\x80\\x9ceastern conference potential all stars but not making it in the West\\xe2\\x80\\x9d group.'\n",
            "\n",
            "Example 1:\n",
            "dst: b\"Kameron est-elle un peu aigrie de son manque de temps \\xc3\\xa0 l'\\xc3\\xa9cran ?\"\n",
            "src: b'Is Kameron a Little Salty About Her Lack of Air Time?'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ds = tfds.load(\"mtnt/en-fr\", split=\"train\")\n",
        "\n",
        "ds = ds.take(2)\n",
        "ds = ds.as_numpy_iterator()\n",
        "\n",
        "for idx, example in enumerate(ds):\n",
        "  print(f'Example {idx}:')\n",
        "  for key, val in example.items():\n",
        "    print(f'{key}: {val}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlY3EYV6jXWR"
      },
      "source": [
        "載入 Gemma 分詞器，由[`sentencepiece.SentencePieceProcessor`](https://github.com/google/sentencepiece/blob/4d6a1f41069c4636c51a5590f7578a0dbed83450/python/src/sentencepiece/__init__.py#L423) 建立：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TpyG5YW1EcoY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.Load(TOKENIZER_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk_CV0A1kR4K"
      },
      "source": [
        "針對英翻法翻譯任務客製化 [`SentencePieceProcessor`](https://github.com/google/sentencepiece/blob/4d6a1f41069c4636c51a5590f7578a0dbed83450/python/src/sentencepiece/__init__.py#L423)。由於你會微調 Gemma 模型的英文區塊，因此需要進行一些調整，例如：\n",
        "\n",
        "* 輸入前置詞彙：在每個輸入前加入共用前置詞彙，標示翻譯任務。例如，你可以使用類似 `將這句話翻譯成法文：[輸入句子]` 的提示，加上前置詞彙。\n",
        "\n",
        "- *翻譯開始字尾詞彙：在每個提示末端加入字尾詞彙，便能精準指示 Gemma 模型開始翻譯的時機。加入換行字元就能完成這個任務。\n",
        "\n",
        "- *語言模型 Token：Gemma 模型預期每個序列的開頭會有 [開頭序列] Token，因此每個訓練範例結尾加入 [結尾序列] Token 就足夠了。\n",
        "\n",
        "按照下列方式建立 `SentencePieceProcessor` 的自訂包裝器：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "L9cjK0uxEcoY"
      },
      "outputs": [],
      "source": [
        "class GemmaTokenizer:\n",
        "\n",
        "  def __init__(self,\n",
        "               spm_processor: spm.SentencePieceProcessor):\n",
        "    self._spm_processor = spm_processor\n",
        "\n",
        "  @property\n",
        "  def pad_id(self) -> int:\n",
        "    \"\"\"Fast access to the pad ID.\"\"\"\n",
        "    return self._spm_processor.pad_id()\n",
        "\n",
        "  def tokenize(self,\n",
        "               example: str | bytes,\n",
        "               prefix: str = '',\n",
        "               suffix: str = '',\n",
        "               add_eos: bool = True) -> jax.Array:\n",
        "    \"\"\"\n",
        "    The tokenization function.\n",
        "\n",
        "    Args:\n",
        "      example: Input string to tokenize.\n",
        "      prefix:  Prefix to add to the input string.\n",
        "      suffix:  Suffix to add to the input string.\n",
        "      add_eos: If True, add an \"end of sentence\" token at the end of the output\n",
        "               sequence.\n",
        "    Returns:\n",
        "      Tokens corresponding to the input string.\n",
        "    \"\"\"\n",
        "    int_list = [self._spm_processor.bos_id()]\n",
        "    int_list.extend(self._spm_processor.EncodeAsIds(prefix + example + suffix))\n",
        "    if add_eos:\n",
        "      int_list.append(self._spm_processor.eos_id())\n",
        "\n",
        "    return jnp.array(int_list, dtype=jnp.int32)\n",
        "\n",
        "  def tokenize_tf_op(self,\n",
        "                     str_tensor: tf.Tensor,\n",
        "                     prefix: str = '',\n",
        "                     suffix: str = '',\n",
        "                     add_eos: bool = True) -> tf.Tensor:\n",
        "    \"\"\"A TensorFlow operator for the tokenize function.\"\"\"\n",
        "    encoded = tf.numpy_function(\n",
        "        self.tokenize,\n",
        "        [str_tensor, prefix, suffix, add_eos],\n",
        "        tf.int32)\n",
        "    encoded.set_shape([None])\n",
        "    return encoded\n",
        "\n",
        "  def to_string(self, tokens: jax.Array) -> str:\n",
        "    \"\"\"Convert an array of tokens to a string.\"\"\"\n",
        "    return self._spm_processor.EncodeIds(tokens.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-oJ2ziwxG1L"
      },
      "source": [
        "嘗試透過例證化新的自訂「GemmaTokenizer」，然後套用在 MTNT 資料集的小型樣本上來試用看看：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xEA-97ioEcoY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 0:\n",
            "src: [     2  49688    736   1280   6987 235292    108    651   2778    576\n",
            "   1080 104745  11982   5736    832   8995    901    780   3547    665\n",
            "    575    573   4589 235369   2778 235265    108]\n",
            "dst: [     2   2025  29653    581    664  16298   1437  55563  41435   7840\n",
            "    581    683 111452    581    533 235303   9776   4108   2459    679\n",
            "    485 235303    479   6728    579   1806   2499    709  29653    581\n",
            "    533 235303 101323  16054      1]\n",
            "\n",
            "Example 1:\n",
            "src: [     2  49688    736   1280   6987 235292    108   2437  87150    477\n",
            "    476  11709 230461   8045   3636  40268    576   4252   4897 235336\n",
            "    108]\n",
            "dst: [     2 213606    477   1455 235290   3510    748   8268 191017   2809\n",
            "    581   2032  69972    581  11495   1305    533 235303  65978   1654\n",
            "      1]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GemmaTokenizer(vocab)\n",
        "\n",
        "def tokenize_source(tokenizer, example: tf.Tensor):\n",
        "  return tokenizer.tokenize_tf_op(example,\n",
        "                                  prefix='Translate this into French:\\n',\n",
        "                                  suffix='\\n',\n",
        "                                  add_eos=False)\n",
        "def tokenize_destination(tokenizer, example: tf.Tensor):\n",
        "  return tokenizer.tokenize_tf_op(example,\n",
        "                                  add_eos=True)\n",
        "\n",
        "ds = tfds.load(\"mtnt/en-fr\",split=\"train\")\n",
        "ds = ds.take(2)\n",
        "ds = ds.map(lambda x: {'src': tokenize_source(tokenizer, x['src']),\n",
        "                       'dst': tokenize_destination(tokenizer, x['dst'])})\n",
        "ds = ds.as_numpy_iterator()\n",
        "\n",
        "for idx, example in enumerate(ds):\n",
        "  print(f'Example {idx}:')\n",
        "  for key, val in example.items():\n",
        "    print(f'{key}: {val}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkY_hThVkkqF"
      },
      "source": [
        "建構一個針對整個 MTNT 資料集的資料載入器：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Zm30Q2lnknmG"
      },
      "outputs": [],
      "source": [
        "@chex.dataclass(frozen=True)\n",
        "class TrainingInput:\n",
        "  # Input tokens provided to the model.\n",
        "  input_tokens: jax.Array\n",
        "\n",
        "  # A mask that determines which tokens contribute to the target loss\n",
        "  # calculation.\n",
        "  target_mask: jax.Array\n",
        "\n",
        "class DatasetSplit(enum.Enum):\n",
        "  TRAIN = 'train'\n",
        "  VALIDATION = 'valid'\n",
        "\n",
        "class MTNTDatasetBuilder:\n",
        "  \"\"\"The dataset builder for the MTNT dataset.\"\"\"\n",
        "\n",
        "  N_ITEMS = {DatasetSplit.TRAIN: 35_692,\n",
        "             DatasetSplit.VALIDATION: 811}\n",
        "\n",
        "  BUFFER_SIZE_SHUFFLE = 10_000\n",
        "  TRANSLATION_PREFIX = 'Translate this into French:\\n'\n",
        "  TRANSLATION_SUFFIX = '\\n'\n",
        "\n",
        "  def __init__(self,\n",
        "               tokenizer : GemmaTokenizer,\n",
        "               max_seq_len: int):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      tokenizer: Gemma tokenizer to use.\n",
        "      max_seq_len: size of each sequence in a given batch.\n",
        "    \"\"\"\n",
        "    self._tokenizer = tokenizer\n",
        "    self._base_data = {\n",
        "        DatasetSplit.TRAIN: tfds.load(\"mtnt/en-fr\",split=\"train\"),\n",
        "        DatasetSplit.VALIDATION: tfds.load(\"mtnt/en-fr\",split=\"valid\"),\n",
        "    }\n",
        "    self._max_seq_len = max_seq_len\n",
        "\n",
        "  def _tokenize_source(self, example: tf.Tensor):\n",
        "    \"\"\"Tokenization function for the source.\"\"\"\n",
        "    return self._tokenizer.tokenize_tf_op(example,\n",
        "                                          prefix=self.TRANSLATION_PREFIX,\n",
        "                                          suffix=self.TRANSLATION_SUFFIX,\n",
        "                                          add_eos=False)\n",
        "\n",
        "  def _tokenize_destination(self, example: tf.Tensor):\n",
        "    \"\"\"Tokenization function for the French translation.\"\"\"\n",
        "    return self._tokenizer.tokenize_tf_op(example,\n",
        "                                          add_eos=True)\n",
        "\n",
        "  def _pad_up_to_max_len(self,\n",
        "                         input_tensor: tf.Tensor,\n",
        "                         pad_value: int | bool,\n",
        "                         ) -> tf.Tensor:\n",
        "    \"\"\"Pad the given tensor up to sequence length of a batch.\"\"\"\n",
        "    seq_len = tf.shape(input_tensor)[0]\n",
        "    to_pad = tf.maximum(self._max_seq_len - seq_len, 0)\n",
        "    return tf.pad(input_tensor,\n",
        "                  [[0, to_pad]],\n",
        "                  mode='CONSTANT',\n",
        "                  constant_values=pad_value,\n",
        "                  )\n",
        "\n",
        "  def _to_training_input(self,\n",
        "                         src_tokens: jax.Array,\n",
        "                         dst_tokens: jax.Array,\n",
        "                         ) -> TrainingInput:\n",
        "    \"\"\"Build a training input from a tuple of source and destination tokens.\"\"\"\n",
        "\n",
        "    # The input sequence fed to the model is simply the concatenation of the\n",
        "    # source and the destination.\n",
        "    tokens = tf.concat([src_tokens, dst_tokens], axis=0)\n",
        "\n",
        "    # To prevent the model from updating based on the source (input)\n",
        "    # tokens, add a target mask to each input.\n",
        "    q_mask = tf.zeros_like(src_tokens, dtype=tf.bool)\n",
        "    a_mask = tf.ones_like(dst_tokens, dtype=tf.bool)\n",
        "    mask = tf.concat([q_mask, a_mask], axis=0)\n",
        "\n",
        "    # If the output tokens sequence is smaller than the target sequence size,\n",
        "    # then pad it with pad tokens.\n",
        "    tokens = self._pad_up_to_max_len(tokens, self._tokenizer.pad_id)\n",
        "\n",
        "    # Don't want to perform the backward pass on the pad tokens.\n",
        "    mask = self._pad_up_to_max_len(mask, False)\n",
        "\n",
        "    return TrainingInput(input_tokens=tokens, target_mask=mask)\n",
        "\n",
        "\n",
        "  def get_train_dataset(self, batch_size: int, num_epochs: int):\n",
        "    \"\"\"Build the training dataset.\"\"\"\n",
        "\n",
        "    # Tokenize each sample.\n",
        "    ds = self._base_data[DatasetSplit.TRAIN].map(lambda x : (self._tokenize_source(x['src']),\n",
        "                                                             self._tokenize_destination(x['dst'])))\n",
        "\n",
        "    # Convert the samples to training inputs.\n",
        "    ds = ds.map(lambda x, y: self._to_training_input(x, y))\n",
        "\n",
        "    # Remove the samples that are too long.\n",
        "    ds = ds.filter(lambda x: tf.shape(x.input_tokens)[0] <= self._max_seq_len)\n",
        "\n",
        "    # Shuffle the dataset.\n",
        "    ds = ds.shuffle(buffer_size=self.BUFFER_SIZE_SHUFFLE)\n",
        "\n",
        "    # Repeat if necessary.\n",
        "    ds = ds.repeat(num_epochs)\n",
        "\n",
        "    # Build batches.\n",
        "    ds = ds.batch(batch_size, drop_remainder=True)\n",
        "    return ds\n",
        "\n",
        "  def get_validation_dataset(self, batch_size: int):\n",
        "    \"\"\"Build the validation dataset.\"\"\"\n",
        "\n",
        "    # Same steps as in `get_train_dataset`, but without shuffling and no repetition.\n",
        "    ds = self._base_data[DatasetSplit.VALIDATION].map(lambda x : (self._tokenize_source(x['src']),\n",
        "                                                                  self._tokenize_destination(x['dst'])))\n",
        "    ds = ds.map(lambda x, y: self._to_training_input(x, y))\n",
        "    ds = ds.filter(lambda x: tf.shape(x.input_tokens)[0] <= self._max_seq_len)\n",
        "    ds = ds.batch(batch_size, drop_remainder=True)\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3jRNKosyLUK"
      },
      "source": [
        "再次執行自訂 `GemmaTokenizer`，接著把它套用到 `MTNT` 資料集上，並抽樣兩個範例，來嘗試 `MTNTDatasetBuilder`：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bYeduOaNEcoZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n",
            "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n",
            "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 0:\n",
            "input_tokens: [[     2  49688    736   1280   6987 235292    108  10924    665  12302\n",
            "  235341    108      2   4397  63011   1437  38696   1241      1      0]\n",
            " [     2  49688    736   1280   6987 235292    108  13835   1517 235265\n",
            "     108      2  69875    540  19713 235265      1      0      0      0]\n",
            " [     2  49688    736   1280   6987 235292    108   6956   1586 235297\n",
            "  235265    108      2  78368   1586 235297 235265      1      0      0]]\n",
            "target_mask: [[False False False False False False False False False False False False\n",
            "   True  True  True  True  True  True  True False]\n",
            " [False False False False False False False False False False False  True\n",
            "   True  True  True  True  True False False False]\n",
            " [False False False False False False False False False False False False\n",
            "   True  True  True  True  True  True False False]]\n",
            "\n",
            "Example 1:\n",
            "input_tokens: [[     2  49688    736   1280   6987 235292    108  18874 235341    108\n",
            "       2 115905   6425   1241      1      0      0      0      0      0]\n",
            " [     2  49688    736   1280   6987 235292    108   7574   3356 235341\n",
            "     108      2   7997  20707   1241      1      0      0      0      0]\n",
            " [     2  49688    736   1280   6987 235292    108   8703    665 235265\n",
            "     108      2 235338 235303  90006  20133 235265      1      0      0]]\n",
            "target_mask: [[False False False False False False False False False False  True  True\n",
            "   True  True  True False False False False False]\n",
            " [False False False False False False False False False False False  True\n",
            "   True  True  True  True False False False False]\n",
            " [False False False False False False False False False False False  True\n",
            "   True  True  True  True  True  True False False]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GemmaTokenizer(vocab)\n",
        "\n",
        "dataset_builder = MTNTDatasetBuilder(tokenizer, max_seq_len=20)\n",
        "ds = dataset_builder.get_train_dataset(3, 1)\n",
        "ds = ds.take(2)\n",
        "ds = ds.as_numpy_iterator()\n",
        "\n",
        "for idx, example in enumerate(ds):\n",
        "  print(f'Example {idx}:')\n",
        "  for key, val in example.items():\n",
        "    print(f'{key}: {val}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IY8Muu1zRF4"
      },
      "source": [
        "## 配置模型\n",
        "\n",
        "在你開始微調 Gemma 模型之前，你需要對其進行配置。\n",
        "\n",
        "首先，使用 [gemma.params.load_and_format_params](https://github.com/google-deepmind/gemma/blob/c6bd156c246530e1620a7c62de98542a377e3934/gemma/params.py#L27) 方法載入並格式化 Gemma 模型檢查點：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "by6eWKtqzxRf"
      },
      "outputs": [],
      "source": [
        "params = params_lib.load_and_format_params(CKPT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtJhJkkZzsy1"
      },
      "source": [
        "要自動從 Gemma 模型檢查點載入正確的組態，請使用 [`gemma.transformer.TransformerConfig`](https://github.com/google-deepmind/gemma/blob/56e501ce147af4ea5c23cc0ddf5a9c4a6b7bd0d0/gemma/transformer.py#L65)。`cache_size` 參數是 Gemma `Transformer` 快取中的時間步數。之後使用 [`gemma.transformer.Transformer`](https://github.com/google-deepmind/gemma/blob/56e501ce147af4ea5c23cc0ddf5a9c4a6b7bd0d0/gemma/transformer.py#L136) 將 Gemma 模型實例化為 `model_2b` (繼承自 [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html))。\n",
        "\n",
        "**註：** 由於在目前版本的 Gemma 中有未使用的 Token，因此詞彙量會小於輸入嵌入的數量。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_jjlFAkazzit"
      },
      "outputs": [],
      "source": [
        "config_2b = transformer_lib.TransformerConfig.from_params(\n",
        "    params,\n",
        "    cache_size=30\n",
        ")\n",
        "\n",
        "model_2b = transformer_lib.Transformer(config=config_2b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7UL2Af536x_"
      },
      "source": [
        "## 微調模型\n",
        "\n",
        "在本節中，你將：\n",
        "\n",
        "- 使用 `gemma.transformer.Transformer` 類別建立正向傳遞和損失函式。\n",
        "- 建構 Token 的位置和注意力遮罩向量\n",
        "- 使用 Flax 建構訓練步驟函式。\n",
        "- 建構沒有反向傳遞的驗證步驟。\n",
        "- 建立訓練迴圈。\n",
        "- 微調 Gemma 模型。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJhtJumH7H8_"
      },
      "source": [
        "使用 [`gemma.transformer.Transformer`](https://github.com/google-deepmind/gemma/blob/56e501ce147af4ea5c23cc0ddf5a9c4a6b7bd0d0/gemma/transformer.py#L136) 類別定義前向傳遞和損失函式。Gemma `Transformer` 從 [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) 繼承，且提供兩個必要的 method：\n",
        "\n",
        "- `init`：初始化模型參數。\n",
        "- `apply`：使用給定的參數組執行模型的 `__call__` 函式。\n",
        "\n",
        "由於你使用預訓練過的 Gemma 權重，不需要使用 `init` 函式。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iEcV0XEEEcoZ"
      },
      "outputs": [],
      "source": [
        "def forward_and_loss_fn(params,\n",
        "                        *,\n",
        "                        model: transformer_lib.Transformer,\n",
        "                        input_tokens: jax.Array,            # Shape [B, L]\n",
        "                        input_mask: jax.Array,              # Shape [B, L]\n",
        "                        positions: jax.Array,               # Shape [B, L]\n",
        "                        attention_mask: jax.Array,          # [B, L, L]\n",
        "                        ) -> jax.Array:\n",
        "  \"\"\"The forward pass and the loss function.\n",
        "\n",
        "  Args:\n",
        "    params: Model's input parameters.\n",
        "    model: The Gemma transformer model to call.\n",
        "    input_tokens: Input tokens sequence, shape [B, L].\n",
        "    input_mask: Tokens to ignore when computing the loss, shape [B, L].\n",
        "    positions: Relative position of each token, shape [B, L].\n",
        "    attention_mask: Input attention mask, shape [B, L].\n",
        "\n",
        "  Returns:\n",
        "    The softmax cross-entropy loss for the next-token prediction task.\n",
        "  \"\"\"\n",
        "\n",
        "  # The forward pass on the input data.\n",
        "  # No attention cache is needed here.\n",
        "  logits, _ = model.apply(\n",
        "        params,\n",
        "        input_tokens,\n",
        "        positions,\n",
        "        None,              # Attention cache is None.\n",
        "        attention_mask,\n",
        "    )\n",
        "\n",
        "  # Exclude the last step as it does not appear in the targets.\n",
        "  logits = logits[0, :-1]\n",
        "\n",
        "  # Similarly, the first token cannot be predicted.\n",
        "  target_tokens = input_tokens[0, 1:]\n",
        "  target_mask = input_mask[0, 1:]\n",
        "\n",
        "  # Convert the target labels to one-hot encoded vectors.\n",
        "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
        "\n",
        "  # Don't update on unwanted tokens.\n",
        "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[...,None]\n",
        "\n",
        "  # Define the normalization factor.\n",
        "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
        "\n",
        "  # Return the negative log likelihood (NLL) loss.\n",
        "  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxbxsKcd7Ot7"
      },
      "source": [
        "[`gemma.transformer.Transformer`](https://github.com/google-deepmind/gemma/blob/56e501ce147af4ea5c23cc0ddf5a9c4a6b7bd0d0/gemma/transformer.py#L136) 類別除了輸入外，還需要一個 `attention_mask` 和一個 `positions` 向量。你可以建立一個自訂函式來產生這些，而此函式會使用 [`Transformer.build_positions_from_mask`](https://github.com/google-deepmind/gemma/blob/56e501ce147af4ea5c23cc0ddf5a9c4a6b7bd0d0/gemma/transformer.py#L48) 和 [`Transformer.make_causal_attn_mask`](https://github.com/google-deepmind/gemma/blob/56e501ce147af4ea5c23cc0ddf5a9c4a6b7bd0d0/gemma/transformer.py#L29)：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cbWfdHf0EcoZ"
      },
      "outputs": [],
      "source": [
        "def get_attention_mask_and_positions(example: jax.Array,\n",
        "                                     pad_id : int,\n",
        "                                     )-> tuple[jax.Array, jax.Array]:\n",
        "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
        "  pad_mask = example != pad_id\n",
        "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
        "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
        "  return current_token_position, attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRkeF6ed8tOI"
      },
      "source": [
        "建立 `train_step` 函式以進行反向傳遞並相應地更新模型的參數，其中：\n",
        "\n",
        "- [`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.value_and_grad.html) 用於在正向和反向傳遞中評估損失函式和梯度。\n",
        "- [`optax.apply_updates`](https://optax.readthedocs.io/en/latest/api/apply_updates.html#optax.apply_updates) 用於更新參數。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cPSfp7ZUEcoZ"
      },
      "outputs": [],
      "source": [
        "def train_step(model: transformer_lib.Transformer,\n",
        "               params,\n",
        "               optimizer: optax.GradientTransformation,\n",
        "               opt_state: optax.OptState,\n",
        "               pad_id: int,\n",
        "               example: TrainingInput):\n",
        "  \"\"\"Train step.\n",
        "\n",
        "  Args:\n",
        "    model: The Gemma transformer model.\n",
        "    params: The model's input parameters.\n",
        "    optimizer: The Optax optimizer to use.\n",
        "    opt_state: The input optimizer's state.\n",
        "    pad_id: ID of the pad token.\n",
        "    example: Input batch.\n",
        "\n",
        "  Returns:\n",
        "    The training loss, the updated parameters, and the updated optimizer state.\n",
        "  \"\"\"\n",
        "\n",
        "  # Build the position and attention mask vectors.\n",
        "  positions, attention_mask = get_attention_mask_and_positions(example.input_tokens, pad_id)\n",
        "\n",
        "  # The forward and backward passes.\n",
        "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(params,\n",
        "                                                             model=model,\n",
        "                                                             input_tokens=example.input_tokens,\n",
        "                                                             input_mask=example.target_mask,\n",
        "                                                             positions=positions,\n",
        "                                                             attention_mask=attention_mask)\n",
        "  # Update the parameters.\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "\n",
        "  return train_loss, params, opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZKSa-jJ809n"
      },
      "source": [
        "建立不具有反向傳遞的 `validation_step` 函式：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yU4oR92YEcoa"
      },
      "outputs": [],
      "source": [
        "def validation_step(model: transformer_lib.Transformer,\n",
        "                    params,\n",
        "                    pad_id: int,\n",
        "                    example: TrainingInput,\n",
        "                    ):\n",
        "  positions, attention_mask = get_attention_mask_and_positions(example.input_tokens, pad_id)\n",
        "  val_loss = forward_and_loss_fn(params,\n",
        "                                 model=model,\n",
        "                                 input_tokens=example.input_tokens,\n",
        "                                 input_mask=example.target_mask,\n",
        "                                 positions=positions,\n",
        "                                 attention_mask=attention_mask)\n",
        "  return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNqVhj7v87f4"
      },
      "source": [
        "使用 [`optax.sgd`](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.sgd) 定義 SGD 優化器的訓練迴圈：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xT4bAqNLEcoa"
      },
      "outputs": [],
      "source": [
        "@chex.dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "  learning_rate: float\n",
        "  num_epochs: int\n",
        "  eval_every_n: int\n",
        "  batch_size: int\n",
        "  max_steps: int | None = None\n",
        "\n",
        "def train_loop(\n",
        "    model: transformer_lib.Transformer,\n",
        "    params,\n",
        "    dataset_builder: MTNTDatasetBuilder,\n",
        "    training_cfg: TrainingConfig):\n",
        "\n",
        "  # Apply `jax.jit` on the training step, making the whole loop much more efficient.\n",
        "  compiled_train_step = jax.jit(train_step, static_argnames=['model', 'optimizer'])\n",
        "\n",
        "  # Apply `jax.jit` on the validation step.\n",
        "  compiled_validation_step = jax.jit(validation_step, static_argnames=['model'])\n",
        "\n",
        "  # To save memory, use the SGD optimizer instead of the usual Adam optimizer.\n",
        "  # Note that for this specific example, SGD is more than enough.\n",
        "  optimizer = optax.sgd(training_cfg.learning_rate)\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  # Build the training dataset.\n",
        "  train_ds = dataset_builder.get_train_dataset(batch_size=training_cfg.batch_size,\n",
        "                                               num_epochs=training_cfg.num_epochs)\n",
        "  train_ds = train_ds.as_numpy_iterator()\n",
        "\n",
        "  # Build the validation dataset, with a limited number of samples for this demo.\n",
        "  validation_ds = dataset_builder.get_validation_dataset(batch_size=training_cfg.batch_size)\n",
        "  validation_ds = validation_ds.take(50)\n",
        "\n",
        "  n_steps = 0\n",
        "  avg_loss=0\n",
        "\n",
        "  # A first round of the validation loss.\n",
        "  n_steps_eval = 0\n",
        "  eval_loss = 0\n",
        "  val_iterator = validation_ds.as_numpy_iterator()\n",
        "  for val_example in val_iterator:\n",
        "    eval_loss += compiled_validation_step(model,\n",
        "                                          params,\n",
        "                                          dataset_builder._tokenizer.pad_id,\n",
        "                                          val_example)\n",
        "    n_steps_eval += 1\n",
        "  print(f\"Start, validation loss: {eval_loss/n_steps_eval}\")\n",
        "\n",
        "  for train_example in train_ds:\n",
        "    train_loss, params, opt_state = compiled_train_step(model=model,\n",
        "                                                        params=params,\n",
        "                                                        optimizer=optimizer,\n",
        "                                                        opt_state=opt_state,\n",
        "                                                        pad_id=dataset_builder._tokenizer.pad_id,\n",
        "                                                        example=train_example)\n",
        "    n_steps += 1\n",
        "    avg_loss += train_loss\n",
        "    if n_steps % training_cfg.eval_every_n == 0:\n",
        "      eval_loss = 0\n",
        "\n",
        "      n_steps_eval = 0\n",
        "      val_iterator = validation_ds.as_numpy_iterator()\n",
        "      for val_example in val_iterator:\n",
        "        eval_loss += compiled_validation_step(model,\n",
        "                                              params,\n",
        "                                              dataset_builder._tokenizer.pad_id,\n",
        "                                              val_example)\n",
        "        n_steps_eval +=1\n",
        "      avg_loss /= training_cfg.eval_every_n\n",
        "      eval_loss /= n_steps_eval\n",
        "      print(f\"STEP {n_steps} training loss: {avg_loss} - eval loss: {eval_loss}\")\n",
        "      avg_loss=0\n",
        "    if training_cfg.max_steps is not None and n_steps > training_cfg.max_steps:\n",
        "      break\n",
        "  return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecv6lp5MCzFc"
      },
      "source": [
        "開始對 Gemma 模型執行微調，並採用有限次的步驟 (`SEQ_SIZE`)，以確保這適合記憶體：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7SL2VAmVEcoa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start, validation loss: 10.647212982177734\n",
            "STEP 20 training loss: 3.3015992641448975 - eval loss: 2.686880111694336\n",
            "STEP 40 training loss: 5.375057220458984 - eval loss: 2.6751961708068848\n",
            "STEP 60 training loss: 2.6599338054656982 - eval loss: 2.663877010345459\n",
            "STEP 80 training loss: 4.822389125823975 - eval loss: 2.3333375453948975\n",
            "STEP 100 training loss: 2.0131142139434814 - eval loss: 2.360811948776245\n"
          ]
        }
      ],
      "source": [
        "SEQ_SIZE = 25\n",
        "tokenizer = GemmaTokenizer(vocab)\n",
        "dataset_builder= MTNTDatasetBuilder(tokenizer, SEQ_SIZE)\n",
        "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
        "                              num_epochs=1,\n",
        "                              eval_every_n=20,\n",
        "                              batch_size=1,\n",
        "                              max_steps=100)\n",
        "\n",
        "params = train_loop(model=model_2b,\n",
        "                    params={'params': params['transformer']},\n",
        "                    dataset_builder=dataset_builder,\n",
        "                    training_cfg=training_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtfVo3pDDAZV"
      },
      "source": [
        "訓練損失量及驗證損失量應會隨著步驟數降低。\n",
        "\n",
        "使用 [`gemma.sampler.Sampler`](https://github.com/google-deepmind/gemma/blob/56e501ce147af4ea5c23cc0ddf5a9c4a6b7bd0d0/gemma/sampler.py#L88) 建立 `sampler`。它使用 Gemma 模型檢查點與 Token 分詞器。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dQ1oCF10Ecod"
      },
      "outputs": [],
      "source": [
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=model_2b,\n",
        "    vocab=vocab,\n",
        "    params=params['params'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-61KZz7EHiIS"
      },
      "source": [
        "使用 `sampler` 檢查你的模型是否能執行翻譯。 [`gemma.sampler.Sampler`](https://github.com/google-deepmind/gemma/blob/56e501ce147af4ea5c23cc0ddf5a9c4a6b7bd0d0/gemma/sampler.py#L88) 中的 `total_generation_steps` 參數是在產生回應時執行的步驟數。如要確保輸入符合訓練格式，請在結尾使用換行字元加上前置詞「將這段文字翻譯成法文：\\n」。這會讓模型知道要開始翻譯了。\n",
        "\n",
        "**注意：** 由於硬體限制，用在 gemma Transformer 中的訓練參數數量可能無法在本示範中產生「穩定的」結果。\n",
        "\n",
        "**注意：** 若發生記憶體不足的情況，請按一下 **執行時期** > **中斷連線並刪除執行時期** ，然後按 **執行時期** > **執行全部** 。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S5F3fk22Ecod"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"C'est Bonjour, mon nom est Morgane.C'est Bonjour, mon nom est Morgane.\"]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampler(\n",
        "    [\"Translate this into French:\\nHello, my name is Morgane.\\n\"],\n",
        "    total_generation_steps=100,\n",
        "    ).text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jao0Qk-ZIqyD"
      },
      "source": [
        "## 了解更多\n",
        "\n",
        "- 你可以在 GitHub 上了解有關 Google DeepMind [`gemma` 函式庫](https://github.com/google-deepmind/gemma) 的更多資訊，該函式庫包含你在本教學課程中所用模組的 docstring，例如 [`gemma.params`](https://github.com/google-deepmind/gemma/blob/main/gemma/params.py)、\n",
        "[`gemma.transformer`](https://github.com/google-deepmind/gemma/blob/main/gemma/transformer.py) 和\n",
        "[`gemma.sampler`](https://github.com/google-deepmind/gemma/blob/main/gemma/sampler.py)。\n",
        "- 下列函式庫有自己的說明文件網站：[core JAX](https://jax.readthedocs.io)、[Flax](https://flax.readthedocs.io)、[Chex](https://chex.readthedocs.io/en/latest/)、[Optax](https://optax.readthedocs.io/en/latest/) 和 [Orbax](https://orbax.readthedocs.io/)。\n",
        "- sentencepiece 分詞器/去分詞器文件，請查看 [Google 的 `sentencepiece` GitHub 儲存庫](https://github.com/google/sentencepiece)。\n",
        "- `kagglehub` 文件，請查看 [Kaggle's `kagglehub` GitHub 儲存庫](https://github.com/Kaggle/kagglehub) 上的 `README.md`。\n",
        "- 了解如何 [使用 Gemma 模型搭配 Google Cloud Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma)。\n",
        "- 如果你使用 Google Cloud TPU (v3-8 與更新版)，請務必也更新到最新的 `jax[tpu]` 套件 (`!pip install -U jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html`)，重新啟動執行時期，並檢查 `jax` 和 `jaxlib` 版本是否一致 (`!pip list | grep jax`)。這樣可以防止因 `jaxlib` 和 `jax` 版本不一致而產生的 `RuntimeError`。有關更多 JAX 安裝說明，請參閱 [JAX 文件](https://jax.readthedocs.io/en/latest/tutorials/installation.html#install-google-tpu)。\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "jax_finetune.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}