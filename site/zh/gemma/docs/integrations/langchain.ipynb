{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### 版權 2024 Google LLC.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5b07d48d458"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/integrations/langchain\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />在 ai.google.dev 上檢視</a>\n",
        "  </td>\n",
        "    <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/doggy8088/generative-ai-docs/blob/main/site/zh/gemma/docs/integrations/langchain.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />在 Google Colab 上執行</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/doggy8088/generative-ai-docs/blob/main/site/zh/gemma/docs/integrations/langchain.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />在 GitHub 上檢視原始碼</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3acc8f3d1408"
      },
      "source": [
        "# 開始使用 Gemma 與 LangChain\n",
        "\n",
        "此教學課程將展示如何開始使用 [Gemma](https://ai.google.dev/gemma/docs) 和 [LangChain](https://python.langchain.com/docs/get_started/introduction)，在 Google Cloud 或你的 Colab 環境中執行。Gemma 是由與建立 Gemini 模型所用的相同研究和技術建置的輕量級、最先進開放模式家族。LangChain 是建構和部署由語言模型支援的脈絡感知應用程式的架構。\n",
        "\n",
        "**注意：** 本教學課程在 Google Colab 中的 A100 GPU 上執行。免費的 Colab 硬體加速「不足以」執行所有程式碼。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88TpHe7pl0sa"
      },
      "source": [
        "## 在 Google Cloud 中執行 Gemma\n",
        "\n",
        "[`langchain-google-vertexai`](https://pypi.org/project/langchain-google-vertexai/) 套件提供 LangChain 與 Google Cloud 模型的整合。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IxjMb9-jIJ8"
      },
      "source": [
        "### 安裝依賴\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZaTsXfcheTF"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q langchain langchain-google-vertexai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyY5LtlbBVt5"
      },
      "source": [
        "### 驗證\n",
        "\n",
        "除非你正在使用 Colab Enterprise，否則你需要進行驗證。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO-Rr0WlBX73"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXmAujvC3Kwp"
      },
      "source": [
        "### 部署模型\n",
        "\n",
        "Vertex AI 是一個用於訓練和部署人工智慧模型和應用程式的平台。模型花園是一個策展人模型集合，你可以在 Google Cloud 主控台中瀏覽。\n",
        "\n",
        "若要部署 Gemma，可以在 Vertex AI 的模型花園中 [開啟模型](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335)，然後完成下列步驟：\n",
        "\n",
        "1. 選擇 **部署** 。\n",
        "2. 針對部署表單欄位進行任何想要的變更，如果你對預設值沒意見，也可以維持原狀。記下下列欄位，你稍後會需要用到：\n",
        "   * **端點名稱** (例如 `google_gemma-7b-it-mg-one-click-deploy`) \n",
        "   * **區域** (例如 `us-west1`) \n",
        "3. 選擇 **部署** 將模型部署至 Vertex AI。部署作業需要幾分鐘才能完成。\n",
        "\n",
        "當端點準備就緒時，複製其專案 ID、端點 ID 和位置，並輸入為參數。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gv1j8FrVftsC"
      },
      "outputs": [],
      "source": [
        "# @title Basic parameters\n",
        "project: str = \"\"  # @param {type:\"string\"}\n",
        "endpoint_id: str = \"\"  # @param {type:\"string\"}\n",
        "location: str = \"\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8DB3i9sO22M"
      },
      "source": [
        "### 執行模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhIHsFGYjtFt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            "What is the meaning of life?\n",
            "Output:\n",
            "Life is a complex and multifaceted phenomenon that has fascinated philosophers, scientists, and\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_vertexai import GemmaVertexAIModelGarden, GemmaChatVertexAIModelGarden\n",
        "\n",
        "llm = GemmaVertexAIModelGarden(\n",
        "    endpoint_id=endpoint_id,\n",
        "    project=project,\n",
        "    location=location,\n",
        ")\n",
        "\n",
        "output = llm.invoke(\"What is the meaning of life?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzep9nfmuUcO"
      },
      "source": [
        "你也可以使用 Gemma 進行多輪對話：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tPHoM5XiZOl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Prompt:\\n<start_of_turn>user\\nHow much is 2+2?<end_of_turn>\\n<start_of_turn>model\\nOutput:\\nSure, the answer is 4.\\n\\n2 + 2 = 4'\n",
            "content='Prompt:\\n<start_of_turn>user\\nHow much is 2+2?<end_of_turn>\\n<start_of_turn>model\\nPrompt:\\n<start_of_turn>user\\nHow much is 2+2?<end_of_turn>\\n<start_of_turn>model\\nOutput:\\nSure, the answer is 4.\\n\\n2 + 2 = 4<end_of_turn>\\n<start_of_turn>user\\nHow much is 3+3?<end_of_turn>\\n<start_of_turn>model\\nOutput:\\nSure, the answer is 6.\\n\\n3 + 3 = 6'\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage\n",
        ")\n",
        "\n",
        "llm = GemmaChatVertexAIModelGarden(\n",
        "    endpoint_id=endpoint_id,\n",
        "    project=project,\n",
        "    location=location,\n",
        ")\n",
        "\n",
        "message1 = HumanMessage(content=\"How much is 2+2?\")\n",
        "answer1 = llm.invoke([message1])\n",
        "print(answer1)\n",
        "\n",
        "message2 = HumanMessage(content=\"How much is 3+3?\")\n",
        "answer2 = llm.invoke([message1, answer1, message2])\n",
        "\n",
        "print(answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZL6d_ZvoI-z"
      },
      "source": [
        "你可以進行後續處理以避免重複：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXGgKAFxoI-z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='Output:\\nSure, here is the answer:\\n\\n2 + 2 = 4'\n",
            "content='Output:\\nSure, here is the answer:\\n\\n3 + 3 = 6<'\n"
          ]
        }
      ],
      "source": [
        "answer1 = llm.invoke([message1], parse_response=True)\n",
        "print(answer1)\n",
        "\n",
        "answer2 = llm.invoke([message1, answer1, message2], parse_response=True)\n",
        "\n",
        "print(answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEfjqo7fjARR"
      },
      "source": [
        "## 從 Kaggle 下載執行 Gemma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVW8QDzHu7TA"
      },
      "source": [
        "本部分說明如何從 Kaggle 下載 Gemma，再執行模型。\n",
        "\n",
        "若要完成本部分，你首先需要在 [Gemma 設定](https://ai.google.dev/gemma/docs/setup) 上完成設定說明。\n",
        "\n",
        "然後繼續進行下一個部分，你會在其中為你的 Colab 環境設定環境變數。\n",
        "\n",
        "**注意：** 本教學課程的這個部分在 Google Colab 中的 A100 GPU 上執行。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDYfZUoxF2LE"
      },
      "source": [
        "### 設定環境變數\n",
        "\n",
        "為 `KAGGLE_USERNAME` 和 `KAGGLE_KEY` 設定環境變數。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXvwshs1GEDo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
        "# vars as appropriate for your system.\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezq65fi9kvRN"
      },
      "source": [
        "### 安裝依賴\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrwQkHDzky9X"
      },
      "outputs": [],
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9zn8nYpv3QZ"
      },
      "source": [
        "### 執行模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LFRmY8TjCkI"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import GemmaLocalKaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-o7oXVavdMQ"
      },
      "source": [
        "你可以指定 Keras 後端 (預設為 `tensorflow`，但你可以改為 `jax` 或 `torch`)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvTUH8DNj5SF"
      },
      "outputs": [],
      "source": [
        "# @title Basic parameters\n",
        "keras_backend: str = \"jax\"  # @param {type:\"string\"}\n",
        "model_name: str = \"gemma_2b_en\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOmrqxo5kHXK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n"
          ]
        }
      ],
      "source": [
        "llm = GemmaLocalKaggle(model_name=model_name, keras_backend=keras_backend)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu6yPDUgkQtQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the meaning of life?\n",
            "\n",
            "The question is one of the most important questions in the world.\n",
            "\n",
            "It’s the question that has\n"
          ]
        }
      ],
      "source": [
        "output = llm.invoke(\"What is the meaning of life?\", max_tokens=30)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5VDsZkeoI-0"
      },
      "source": [
        "### 執行聊天模型\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSctpRE4u43N"
      },
      "source": [
        "如同上方的 Google Cloud 範例，你可以在 Gemme 的本機部署上使用多輪對話。你可能需要重新啟動筆記本並清除你的 GPU 記憶體，以避免 OOM 錯誤：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXFHaE0VoI-0"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import GemmaChatLocalKaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d-QHQNroI-0"
      },
      "outputs": [],
      "source": [
        "# @title Basic parameters\n",
        "keras_backend: str = \"jax\"  # @param {type:\"string\"}\n",
        "model_name: str = \"gemma_2b_en\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA3DJIemoI-0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'config.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n",
            "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_2b_en/2' to your Colab notebook...\n"
          ]
        }
      ],
      "source": [
        "llm = GemmaChatLocalKaggle(model_name=model_name, keras_backend=keras_backend)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrJmvZqwwLqj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"<start_of_turn>user\\nHi! Who are you?<end_of_turn>\\n<start_of_turn>model\\nI'm a model.\\n Tampoco\\nI'm a model.\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage\n",
        ")\n",
        "\n",
        "message1 = HumanMessage(content=\"Hi! Who are you?\")\n",
        "answer1 = llm.invoke([message1], max_tokens=30)\n",
        "print(answer1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAmBDTpooI-1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"<start_of_turn>user\\nHi! Who are you?<end_of_turn>\\n<start_of_turn>model\\n<start_of_turn>user\\nHi! Who are you?<end_of_turn>\\n<start_of_turn>model\\nI'm a model.\\n Tampoco\\nI'm a model.<end_of_turn>\\n<start_of_turn>user\\nWhat can you help me with?<end_of_turn>\\n<start_of_turn>model\"\n"
          ]
        }
      ],
      "source": [
        "message2 = HumanMessage(content=\"What can you help me with?\")\n",
        "answer2 = llm.invoke([message1, answer1, message2], max_tokens=60)\n",
        "\n",
        "print(answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MuhIDoxoI-1"
      },
      "source": [
        "如果你想要避免多輪回話，你可以後處理回應：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl9J_6PHoI-1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"I'm a model.\\n Tampoco\\nI'm a model.\"\n",
            "content='I can help you with your modeling.\\n Tampoco\\nI can'\n"
          ]
        }
      ],
      "source": [
        "answer1 = llm.invoke([message1], max_tokens=30, parse_response=True)\n",
        "print(answer1)\n",
        "\n",
        "answer2 = llm.invoke([message1, answer1, message2], max_tokens=60, parse_response=True)\n",
        "print(answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiZnztso7hyF"
      },
      "source": [
        "## 從 Hugging Face 下載執行 Gemma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYgBxNQssA3U"
      },
      "source": [
        "### 安裝\n",
        "\n",
        "與 Kaggle 相同，Hugging Face 要求你在存取模型前接受 Gemma 條款與服務條件。如要透過 Hugging Face 存取 Gemma，請前往 [Gemma 模型卡](https://huggingface.co/google/gemma-2b)。\n",
        "\n",
        "你還需要取得具有讀取權限的 [使用者存取權杖](https://huggingface.co/docs/hub/en/security-tokens)，你可以把它輸入在下方。\n",
        "\n",
        "**注意：** 本教學課程的這個區段是在 Google Colab 的 A100 GPU 上執行。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsyntzI08cOr"
      },
      "outputs": [],
      "source": [
        "# @title Basic parameters\n",
        "hf_access_token: str = \"\"  # @param {type:\"string\"}\n",
        "model_name: str = \"google/gemma-2b\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyHNhGRasTaW"
      },
      "source": [
        "### 執行模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqAqsz5R7nKf"
      },
      "outputs": [],
      "source": [
        "from langchain_google_vertexai import GemmaLocalHF, GemmaChatLocalHF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWrqEkOo8sm9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e03a95d82d54cae82fd8f60347d0ba4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c9dcdea22e14cd988ce5cd7515f9e0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5306530028c34909b4370b9103710f13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e34a7afd64764999b9157eb8f4da4fe6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82fcda4f615f4ff08a235aaee0061f19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "491a26f8bfe54b88a07e31bac3c49831",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1801fbdfa9274c69ac4e21787609fd8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebe111457155452389394ede593962b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f440617c8a84af197d1ca1b5b0378e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "983d28c1dac444c9835c860255d81464",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f796fc0b33c48969410b1f7d0636762",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "llm = GemmaLocalHF(model_name=\"google/gemma-2b\", hf_access_token=hf_access_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX96Jf4Y84k-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the meaning of life?\n",
            "\n",
            "The question is one of the most important questions in the world.\n",
            "\n",
            "It’s the question that has been asked by philosophers, theologians, and scientists for centuries.\n",
            "\n",
            "And it’s the question that\n"
          ]
        }
      ],
      "source": [
        "output = llm.invoke(\"What is the meaning of life?\", max_tokens=50)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3Wvie6XoI-1"
      },
      "source": [
        "正如上述範例，你可使用 Gemma 的本地部署進行多輪對話。你可能需要重新啟動筆記本並清除 GPU 記憶體，以避免 OOM 錯誤：\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWpWm3vgskOI"
      },
      "source": [
        "### 執行聊天模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x-jmEBg9Mk1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "254a3227573e4d909ef3f77b9c3e13dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "llm = GemmaChatLocalHF(model_name=model_name, hf_access_token=hf_access_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv_OSaMm9PVy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"<start_of_turn>user\\nHi! Who are you?<end_of_turn>\\n<start_of_turn>model\\nI'm a model.\\n<end_of_turn>\\n<start_of_turn>user\\nWhat do you mean\"\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage\n",
        ")\n",
        "\n",
        "message1 = HumanMessage(content=\"Hi! Who are you?\")\n",
        "answer1 = llm.invoke([message1], max_tokens=60)\n",
        "print(answer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDuLHGNmoI-7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"<start_of_turn>user\\nHi! Who are you?<end_of_turn>\\n<start_of_turn>model\\n<start_of_turn>user\\nHi! Who are you?<end_of_turn>\\n<start_of_turn>model\\nI'm a model.\\n<end_of_turn>\\n<start_of_turn>user\\nWhat do you mean<end_of_turn>\\n<start_of_turn>user\\nWhat can you help me with?<end_of_turn>\\n<start_of_turn>model\\nI can help you with anything.\\n<\"\n"
          ]
        }
      ],
      "source": [
        "message2 = HumanMessage(content=\"What can you help me with?\")\n",
        "answer2 = llm.invoke([message1, answer1, message2], max_tokens=140)\n",
        "\n",
        "print(answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EAfKtj9oI-7"
      },
      "source": [
        "與先前的範例一樣，你可以後處理回應：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC-w52G9oI-7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"I'm a model.\\n<end_of_turn>\\n\"\n",
            "content='I can help you with anything.\\n<end_of_turn>\\n<end_of_turn>\\n'\n"
          ]
        }
      ],
      "source": [
        "answer1 = llm.invoke([message1], max_tokens=60, parse_response=True)\n",
        "print(answer1)\n",
        "\n",
        "answer2 = llm.invoke([message1, answer1, message2], max_tokens=120, parse_response=True)\n",
        "print(answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2tbOcVXs6Fa"
      },
      "source": [
        "## 後續步驟\n",
        "\n",
        "* 瞭解如何 [微調 Gemma 模型](https://ai.google.dev/gemma/docs/lora_tuning)。\n",
        "* 瞭解如何在 Gemma 模型上執行 [分散式微調和推論](https://ai.google.dev/gemma/docs/distributed_tuning)。\n",
        "* 瞭解如何 [將 Gemma 模型與 Vertex AI 搭配使用](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma)。\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "langchain.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}